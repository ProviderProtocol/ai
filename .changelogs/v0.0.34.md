# v0.0.34 Changelog

## Added

- [x] Added: **Groq provider** - Fast inference with Llama, Gemma, and Mixtral models
- [x] Added: **Cerebras provider** - Ultra-fast inference with Llama, Qwen, and GPT-OSS models

## New Provider: Groq

Full-featured integration with Groq's inference platform.

**Capabilities:**
- Streaming (SSE)
- Tool/function calling
- Structured output (JSON schema)
- Image input (Llama 4 preview models)

```ts
import { groq } from '@providerprotocol/ai/groq';
import { llm } from '@providerprotocol/ai';

const model = llm({
  model: groq('llama-3.3-70b-versatile'),
  params: { max_tokens: 1000 }
});

const turn = await model.generate('Hello!');
```

**Environment:** `GROQ_API_KEY`

## New Provider: Cerebras

Full-featured integration with Cerebras's ultra-fast inference platform, including reasoning support for compatible models.

**Capabilities:**
- Streaming (SSE)
- Tool/function calling
- Structured output (JSON schema)
- Reasoning parameters (`reasoning_effort`, `reasoning_format`) for GPT-OSS models

```ts
import { cerebras } from '@providerprotocol/ai/cerebras';
import { llm } from '@providerprotocol/ai';

const model = llm({
  model: cerebras('llama-3.3-70b'),
  params: { max_completion_tokens: 1000 }
});

const turn = await model.generate('Hello!');
```

**With reasoning (GPT-OSS):**

```ts
const model = llm({
  model: cerebras('gpt-oss-120b'),
  params: {
    reasoning_effort: 'high',
    reasoning_format: 'parsed'
  }
});
```

**Environment:** `CEREBRAS_API_KEY`
