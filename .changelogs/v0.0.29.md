# v0.0.29 Changelog

## OpenResponses Provider

New `responses()` provider implementing the [OpenResponses specification](https://www.openresponses.org) - an open-source standard for multi-provider, interoperable LLM interfaces.

### Features

- [x] Configurable host URL for any OpenResponses-compatible server
- [x] Full multimodal support: text, images, documents, video, audio
- [x] Streaming with semantic SSE events
- [x] Function/tool calling with parallel execution
- [x] Structured output via JSON schema
- [x] Reasoning summaries for gpt-5/o-series models
- [x] Multi-turn conversation context preservation

### Usage

```ts
import { llm } from '@providerprotocol/ai';
import { responses } from '@providerprotocol/ai/responses';

// Using with OpenAI
const model = llm({
  model: responses('gpt-5.2', {
    host: 'https://api.openai.com/v1',
    apiKeyEnv: 'OPENAI_API_KEY'
  }),
  params: { max_output_tokens: 1000 }
});

const turn = await model.generate('Hello!');
```

```ts
// Using with OpenRouter
const routerModel = llm({
  model: responses('openai/gpt-4o', {
    host: 'https://openrouter.ai/api/v1',
    apiKeyEnv: 'OPENROUTER_API_KEY'
  })
});
```

```ts
// Using with self-hosted server
const localModel = llm({
  model: responses('llama-3.3-70b', {
    host: 'http://localhost:8080/v1'
  })
});
```

### Provider Options

```ts
interface ResponsesProviderOptions {
  host: string;      // Required: Base URL (e.g., 'https://api.openai.com/v1')
  apiKeyEnv?: string; // Optional: Env var name (default: 'OPENRESPONSES_API_KEY')
}
```

### Parameters

```ts
interface ResponsesParams {
  max_output_tokens?: number;
  temperature?: number;
  top_p?: number;
  presence_penalty?: number;
  frequency_penalty?: number;
  reasoning?: { effort?: 'none' | 'low' | 'medium' | 'high' | 'xhigh'; summary?: 'auto' | 'concise' | 'detailed' };
  truncation?: 'auto' | 'disabled';
  parallel_tool_calls?: boolean;
  // ... and more
}
```

### New Exports

From `@providerprotocol/ai/responses`:
- `responses` - Provider factory
- `ResponsesProviderOptions`
- `ResponsesParams`
- `ResponsesRequest`
- `ResponsesResponse`
- `ResponsesStreamEvent`
- `ResponsesUsage`
- `ResponsesInputItem`
- `ResponsesOutputItem`
- `ResponsesContentPart`
- `ResponsesFunctionTool`
- `ResponsesBuiltInTool`
- `ResponsesToolUnion`
- `ResponsesHeaders`

### New Files

- `src/providers/responses/types.ts` - Type definitions
- `src/providers/responses/transform.ts` - Request/response transformers
- `src/providers/responses/llm.ts` - LLM handler
- `src/providers/responses/index.ts` - Provider factory
- `src/responses/index.ts` - Re-export entry point

## Tests

- [x] `tests/unit/providers/responses.transform.test.ts` - 15 unit tests for transforms
- [x] `tests/live/responses.live.test.ts` - 13 live tests against OpenAI backend
